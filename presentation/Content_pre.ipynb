{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visiualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, 1000 samples were randomly selected from the original data final_df, and then the pd.cut() function was used to divide the book ratings into three categories: low, medium, and high ratings. This can help us better understand how authors perform at different rating levels.This visualization allows us to visually see the average rating levels of different authors and compare them according to rating categories. This is very helpful for understanding how authors behave under different levels of popularity in the book recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation: \n",
    "First, the 'Book-Title', 'Book-Author', 'Description', and 'Categories' columns in books_df are concatenated into a text corpus. \n",
    "Using Gensim's Word2Vec model, a word vector model word2vec_model_recommender is obtained after training in corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (books_df['Book-Title'].astype(str) + ' ' + \n",
    "          books_df['Book-Author'].astype(str) + ' ' +\n",
    "          books_df['Description'].astype(str) + ' ' +\n",
    "          books_df['Categories'].astype(str)).apply(str.split).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended algorithm Word2Vec:**\n",
    "- The recommend function is the core of the recommendation algorithm. \n",
    "- The input parameters are the user ID user_id, the complete book data, and the trained Word2Vec model word2vec_model. \n",
    "- First get the user's favorite book information, including title, author, category, and description. \n",
    "- This information is spliced into a text and divided into words to obtain an average word vector avg_vector. \n",
    "- Then the average word vector of each book is calculated, and the cosine similarity is calculated with the average word vector of the  user, and the similarity score is obtained. \n",
    "- According to the similarity score, the top 10 books with the highest similarity and scores higher than 5 are selected as the recommended results. \n",
    "- Finally, the recommendation result is returned in DataFrame format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "word2vec_model_recommender = Word2Vec(sentences=corpus, vector_size=500, window=5, min_count=5, sg=2)\n",
    "\n",
    "def recommend(user_id, data, word2vec_model):\n",
    "    # Get user preferences\n",
    "    user_preferences = data[data['User-ID'] == user_id]\n",
    "    if user_preferences.empty:\n",
    "        return None\n",
    "\n",
    "    # Get information about your favorite books\n",
    "    liked_books = user_preferences['Book-Title'].tolist()\n",
    "    liked_authors = user_preferences['Book-Author'].tolist()\n",
    "    liked_genres = user_preferences['Categories'].tolist()\n",
    "    liked_description = user_preferences['Description'].tolist()\n",
    "\n",
    "    # Merge user preference information\n",
    "    text = ' '.join(liked_books + liked_authors + liked_genres + liked_description)\n",
    "    \n",
    "    # Divide the text into words\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Get text vector\n",
    "    vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return None\n",
    "    avg_vector = sum(vectors) / len(vectors)\n",
    "\n",
    "    # Calculate the similarity to each book\n",
    "    similarities = []\n",
    "    recommended_titles = set()\n",
    "    for idx, row in data.iterrows():\n",
    "        row_text = ' '.join([str(row[col]) for col in data.columns])\n",
    "        row_tokens = row_text.split()\n",
    "        row_vectors = [word2vec_model.wv[token] for token in row_tokens if token in word2vec_model.wv]\n",
    "        if len(row_vectors) > 0:\n",
    "            row_avg_vector = sum(row_vectors) / len(row_vectors)\n",
    "            similarity = cosine_similarity([avg_vector], [row_avg_vector])[0][0]\n",
    "            similarities.append((row, similarity))\n",
    "\n",
    "    # Rank the similarity and select the top 10 recommendations\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    recommendations = []\n",
    "    for book, sim in similarities:\n",
    "        if book['Book-Title'] not in recommended_titles and book['Book-Rating'] > 5:\n",
    "            recommendations.append(book.to_dict())\n",
    "            recommended_titles.add(book['Book-Title'])\n",
    "        if len(recommendations) >= 10:\n",
    "            break\n",
    "\n",
    "    # Convert the recommendation result to a DataFrame\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage: \n",
    "In the final section, using the example with user ID 9714, call the recommend function for a book recommendation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_id = 9714\n",
    "recommendations = recommend(user_id, final_df, word2vec_model_recommender)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "This Word2VeC-based recommendation algorithm uses the text information of books (title, author, description, and category) to capture semantic correlations between books by learning word vectors. When the user likes some books, the system can find similar books to recommend according to the characteristics of these books. This approach can provide more personalized and semantically relevant recommendations, with better performance compared to simple popularity-based or collaborative filtering recommendation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Splicing the 'Book Title', 'Book Author', 'Description' and 'Categories' columns in' final_df 'into a large text corpus. \n",
    "Use 'TfidfVectorizer' to extract TF-IDF features from 'corpus' and generate tfidf_matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation Algorithm**:\n",
    "- The 'recommend' function accepts the user ID and the entire dataset 'final_df' as input. \n",
    "- First, get the user's favorite book list 'user_preferences'. \n",
    "- Then, generate the user preference vector user_vector according to user_preferences. \n",
    "- Next, traverse the entire data set, calculating the cosine similarity between each book and the user's preference vector, and store the results in the similarities list. \n",
    "- Sorts the similarities list in descending order of similarity. \n",
    "- From the similarities list after sorting, select books that the first 10 users are not familiar with, score more than 5 points, and do not repeat as the recommendation results. \n",
    "- Finally, the recommendation result is returned in DataFrame format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def recommend(user_id, data):\n",
    "    # Get user preferences\n",
    "    user_preferences = final_df.loc[final_df['User-ID'] == user_id, 'Book-Title'].tolist()\n",
    "    \n",
    "    # Create a user preference vector\n",
    "    user_vector = tfidf_vectorizer.transform([' '.join(user_preferences)])\n",
    "    \n",
    "    # Calculate how similar each book is to user preferences\n",
    "    similarities = []\n",
    "    for idx, row in data.iterrows():\n",
    "        row_text = ' '.join([str(row[col]) for col in data.columns])\n",
    "        row_vector = tfidf_vectorizer.transform([row_text])\n",
    "        similarity = cosine_similarity(user_vector, row_vector)[0][0]\n",
    "        similarities.append((row, similarity))\n",
    "    \n",
    "    # Sort in descending order of similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 recommendations\n",
    "    recommendations = []\n",
    "    recommended_titles = set()\n",
    "    for book, sim in similarities:\n",
    "        if book['Book-Title'] not in user_preferences and book['Book-Rating'] > 5 and book['Book-Title'] not in recommended_titles:\n",
    "            recommendations.append(book.to_dict())\n",
    "            recommended_titles.add(book['Book-Title'])\n",
    "        if len(recommendations) >= 10:\n",
    "            break\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "    return recommendations_df\n",
    "\n",
    "# 输入用户编号\n",
    "user_id = 9714\n",
    "recommendations = recommend(user_id, final_df)\n",
    "recommendations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
