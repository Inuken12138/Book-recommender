{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYyHxi73wW_q"
      },
      "source": [
        "Project name: Book recommender system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b22EKhV_xgxd"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXjXi6oCwdVq"
      },
      "source": [
        "### Methodology\n",
        "**Conten-based:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoSvx789zyVi"
      },
      "source": [
        "**Collaborative Filtering**\n",
        "\n",
        "Since we have users 'interaction' with books - theirs ratings, we can use collaborative filtering using these interactions, based on the idea that users who have agreed in the past will agree in the future.\n",
        "\n",
        "In our project, Alternating Least Squares (ALS) algorithm is used to identify the patterns in both users and books. ALS can factorize the large user-item interaction matrix into two lower-dimensional matrices that capture the latent factors of users and items. The final goal of ALS is to minimize the difference between the actual ratings and the predicted ones derived from the latent factors.\n",
        "\n",
        "\n",
        "1. Data preparation \n",
        "\n",
        "First, the user and book IDs are mapped to a continuous range of indices, which is essential for creating a sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "books = pd.read_csv('Books.csv')\n",
        "users = pd.read_csv('Users.csv')\n",
        "ratings = pd.read_csv('Ratings.csv')\n",
        "\n",
        "\n",
        "# Map user and book IDs to continuous indices\n",
        "user_id_mapping = {id: idx for idx, id in enumerate(ratings['User-ID'].unique())}\n",
        "book_id_mapping = {id: idx for idx, id in enumerate(ratings['ISBN'].unique())}\n",
        "\n",
        "ratings['User-ID'] = ratings['User-ID'].map(user_id_mapping)\n",
        "ratings['ISBN'] = ratings['ISBN'].map(book_id_mapping)\n",
        "ratings.dropna(subset=['Book-Rating'], inplace=True)\n",
        "all_user_ids = ratings['User-ID'].unique()\n",
        "all_book_ids = ratings['ISBN'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Train-Test Split\n",
        "\n",
        "The data is split into training and test sets. To ensure that all users and books are represented in the training set, any users or books that are only present in the test set are added back to the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "train_user_ids = set(train_data['User-ID'])\n",
        "train_book_ids = set(train_data['ISBN'])\n",
        "missing_users = set(all_user_ids) - train_user_ids\n",
        "missing_books = set(all_book_ids) - train_book_ids\n",
        "\n",
        "missing_data = ratings[ratings['User-ID'].isin(missing_users) | ratings['ISBN'].isin(missing_books)]\n",
        "train_data = pd.concat([train_data, missing_data]).drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Create Sparse Matrices\n",
        "\n",
        "Sparse matrices for the training and test data are created. These matrices are used by the ALS algorithm to learn the latent factors. Here we also added a test to judge whether there is null values in the training matrix, in case during matric factorization there is error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------matrix finished---------\n",
            "No NaN values in training matrix\n"
          ]
        }
      ],
      "source": [
        "# Create sparse matrices for training and test data\n",
        "n_users = ratings['User-ID'].nunique()\n",
        "n_items = ratings['ISBN'].nunique()\n",
        "train_matrix = csr_matrix((train_data['Book-Rating'], (train_data['User-ID'], train_data['ISBN'])), shape=(n_users, n_items))\n",
        "test_matrix = csr_matrix((test_data['Book-Rating'], (test_data['User-ID'], test_data['ISBN'])), shape=(n_users, n_items))\n",
        "print(\"-------matrix finished---------\")\n",
        "\n",
        "# Check for NaN values in the training matrix\n",
        "if np.any(np.isnan(train_matrix.data)):\n",
        "    print(\"NaN values found in training matrix\")\n",
        "else:\n",
        "    print(\"No NaN values in training matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Train the ALS Model\n",
        "\n",
        "An ALS model is initialized and trained using the training data. The model learns latent factors for users and books. Here we set the iteration to be 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
            "  check_blas_config()\n",
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.02100205421447754 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 20/20 [00:23<00:00,  1.17s/it, loss=6.2e-5] \n"
          ]
        }
      ],
      "source": [
        "# Initialize and train the ALS model\n",
        "als_model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20, use_gpu=False, calculate_training_loss=True)\n",
        "als_model.fit(train_matrix.T, show_progress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Evaluate the Model\n",
        "\n",
        "The model is evaluated using the test data by calculating the Root Mean Square Error (RMSE) between the predicted ratings and the actual ratings in the test set.\n",
        "\n",
        "We first extract non-zero entries, then iterate over Non-Zero User-Item pairs to calculate predictions, during this we also check index bounds. After the predictions, we first handle empty predictions, If no predictions were made when the predictions list is empty, the function returns infinity, which is a safeguard to indicate that the model could not make any predictions. Finally RMSE is calculated.\n",
        "\n",
        "In our experiment, Test RMSE is: 7.844746002414569"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def evaluate_model(test_matrix, als_model):\n",
        "    test_user_items = test_matrix.nonzero()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    for user, item in zip(test_user_items[0], test_user_items[1]):\n",
        "        if user < als_model.user_factors.shape[0] and item < als_model.item_factors.shape[0]:\n",
        "            prediction = als_model.user_factors[user, :].dot(als_model.item_factors[item, :].T)\n",
        "            predictions.append(prediction)\n",
        "            ground_truth.append(test_matrix[user, item])\n",
        "    if len(predictions) == 0:\n",
        "        return float('inf')  \n",
        "    return np.sqrt(mean_squared_error(ground_truth, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Generate Recommendations\n",
        "\n",
        "Since the recommendation is based on user's past action, after checking whether user is in our mapping, we retrieve the ratings given by the user from the training matrix, then use it by the recommend method to filter out books that the user has already rated. The recommendation is based on Implicit library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_books_als(user_id, num_recommendations=5):\n",
        "    if user_id not in user_id_mapping:\n",
        "        return [] \n",
        "    user_index = user_id_mapping[user_id]\n",
        "    user_ratings = train_matrix[user_index]\n",
        "    recommended_books = als_model.recommend(user_index, user_ratings, N=num_recommendations, filter_already_liked_items=True)\n",
        "    # recommended_book_ids = [list(book_id_mapping.keys())[list(book_id_mapping.values()).index(i)] for i, _ in recommended_books]\n",
        "    # return books[books['ISBN'].isin(recommended_book_ids)]\n",
        "\n",
        "    # Convert recommended book indices back to ISBNs\n",
        "    recommended_book_isbns = [list(book_id_mapping.keys())[list(book_id_mapping.values()).index(i[0])] for i in recommended_books]\n",
        "    \n",
        "    # Retrieve book information from books DataFrame\n",
        "    recommended_books_info = books[books['ISBN'].isin(recommended_book_isbns)]\n",
        "    \n",
        "    return recommended_books_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Conclusion for ALS:\n",
        "\n",
        "By using ALS, we managed to discover hidden patterns and relationships in the rating data by learning latent factors for both users and books. Also, we managed to reduce the high-dimensional user-item matrix into lower-dimensional ones. The RMSE is relatively low, and we show it effeciency in predicting unknown interactions and generate personalized recommendations for users. Also, we can use it to compute similaritied between users and items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "2.85138e-12 is not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecommend_books_als\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[17], line 11\u001b[0m, in \u001b[0;36mrecommend_books_als\u001b[1;34m(user_id, num_recommendations)\u001b[0m\n\u001b[0;32m      6\u001b[0m recommended_books \u001b[38;5;241m=\u001b[39m als_model\u001b[38;5;241m.\u001b[39mrecommend(user_index, user_ratings, N\u001b[38;5;241m=\u001b[39mnum_recommendations, filter_already_liked_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# recommended_book_ids = [list(book_id_mapping.keys())[list(book_id_mapping.values()).index(i)] for i, _ in recommended_books]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# return books[books['ISBN'].isin(recommended_book_ids)]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert recommended book indices back to ISBNs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m recommended_book_isbns \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbook_id_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbook_id_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecommended_books\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Retrieve book information from books DataFrame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m recommended_books_info \u001b[38;5;241m=\u001b[39m books[books[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISBN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(recommended_book_isbns)]\n",
            "Cell \u001b[1;32mIn[17], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m recommended_books \u001b[38;5;241m=\u001b[39m als_model\u001b[38;5;241m.\u001b[39mrecommend(user_index, user_ratings, N\u001b[38;5;241m=\u001b[39mnum_recommendations, filter_already_liked_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# recommended_book_ids = [list(book_id_mapping.keys())[list(book_id_mapping.values()).index(i)] for i, _ in recommended_books]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# return books[books['ISBN'].isin(recommended_book_ids)]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert recommended book indices back to ISBNs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m recommended_book_isbns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(book_id_mapping\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbook_id_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m recommended_books]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Retrieve book information from books DataFrame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m recommended_books_info \u001b[38;5;241m=\u001b[39m books[books[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISBN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(recommended_book_isbns)]\n",
            "\u001b[1;31mValueError\u001b[0m: 2.85138e-12 is not in list"
          ]
        }
      ],
      "source": [
        "recommend_books_als(100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
