{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOtwthRclAUh"
   },
   "source": [
    "Project name: Book recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhpUZEiSlAUi"
   },
   "source": [
    "Problem:\n",
    "\n",
    "<u>Objective</u>:\n",
    "This project aims to recommendate books to users based on a hybrid of content-based and collaborative-based recommendation system. The problem is given a dataset of users, books and users' ratings, how to recommend most relevant books to our users such that accuracy and relevancy of the recommended books are maximized.\n",
    "\n",
    "<u>Platform used</u>:\n",
    "The system is based on a mobile platform where users can interact with the systems and recommendations are provided upon user login. With the large database of books and users, it makes logistic sense to run the core of the recommendation engine on our own server and the mobile app is only a platform where recommendations are served. The server will handle all recommendations computation such as cosine-similarity match (for content-filtering) and matrix factorization (for collaborative-filtering).\n",
    "\n",
    "### Competitor Analysis\n",
    "\n",
    "**Goodreads:** The largest market share holder by far is Goodreads which has [140 million members (2022)](https://www.goodreads.com/blog/show/2302-goodreads-members-top-72-hit-books-of-the-year-so-far) and [3.5 billion book](https://help.goodreads.com/s/question/0D58V00008Rm69PSAR/how-many-books-are-listed-on-the-goodreads-site). However, a common critism seems to be that the site is very much under-developed and the recommendation algorithm is very \"primitive\". The claim of the latter is caused by the increasingly irrelevant books that are recommended. To verify this, one of the group member did an emperical obversation by registering an account and set up the recommendation to suit his perferance. His perference is based on history and particulary the WW2 era. Upon seeing the recommended books, most of them seem related to WW2 history but a not insignicant number of the recommendations are completely irrelevant to his perference (e.g. a Spiderman comic, a teenage novel etc). From furthure observation, these \"irrelevant\" books are recommended because the user rated some other books that are related to history (Both of these are recommended because the user liked a comic about the concentration camp). The algortihm used seems to be a naive similiartiy matching between the books the user liked and all other books in the dataset.\n",
    "\n",
    "**The StoryGraph:**\n",
    "As a major alternative to Goodreads, The StoryGraph offers a much more personalised recommendation to users by analysing users' reading habits, and break down the online library by mood, pace, length, genre, rating, etc. The site is under heavy development with many short, medium and long-term road map of experimental features. (E.g. a beta feature like tropes or triggers that users want to avoid in recommended books). The feedback on this site is generally positive for it's tailored recommendations. A major limitation of the site is that there are only a few thousands books available and the social/community aspect is not as strong as Goodreads social community.\n",
    "\n",
    "### Our recommendation system:\n",
    "**User input:** Currently, the only source of input for our users would be the N number of book rating (1-10) that new users will be asked upon first registering an account. The ratings are vital for our recommendation system to provide relevant recommendation tailered to the user (detail explanation in the method section). The form of the recommendation is as follow: a top N number of books would be recommended to the user and shown on screen. These are sorted based on the relevancy of these books to the user as determined by our recommendation system. These recommendations are shown to the user upon entering our platform. Recommendation for each user will be recalculated periodically based on user's new book ratings. Again, the only form of feedback from users are the book ratings but more forms of feedback can be considered in the future. This includes: dislike buttons, some specific trigers that users want to avoid, book reviews, feedback for our platform etc. For ideas as to how to implement these, if a user dislikes a book or some type of books, the system should account for this by deleting some of the books in the recommended list or adjust the recommendation model such that these types of books will be heavily penalised inside our model (i.e. a negative score in the matrix factorization model).\n",
    "\n",
    "Since this is a hybrid ensemble model, we will walk through the gist of the model. There are 2 recommendation models and each have deals with a seperate problem.\n",
    "**Content-based:**\n",
    "Firstly, for content-based filtering, the problem is a ranking problem because we will build a user preference vector based on all the books they rated, and this profile is compared against all other books in the datase using tf-idf. All books are then sorted based on their similarity to the user preference vector and top N most similar books will be picked. This is a ranking problem.\n",
    "\n",
    "**Collaborative-based:**\n",
    "Secondly, for Collaborative-based filtering, the problem is both a rating/estimation problem and ranking problem. The goal of this model is to predict the most likely ratings of each user on books they haven't rated. Annd it is also a rannking problem because after getting the predicted book ratings, the predicted ratings need to be sorted for each user and pick the top N books to recommend.\n",
    "\n",
    "**User interface:**\n",
    "\n",
    "N number of recommended books are displayed at a time. Users can click on the book they are interested in to see more details about the book or click on the play button to start reading directly.\n",
    "\n",
    "<img src=\"UX1.png\">\n",
    "\n",
    "Here, the user can see more information about the book. Particularly, the book’s ISBN, year of publication, synopsis and publisher of the book. There is also a book’s rating aggregated from users who read the book. Users can start reading the book by clicking the play button. Users are also allowed to give ratings after reading the book.\n",
    "\n",
    "<img src=\"UX2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECoVfZoElAUj"
   },
   "source": [
    "### Dataset\n",
    "**Basic characteristics:**\n",
    "The dataset we used is mainly from [Kaggle](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data?select=Ratings.csv) The data was collected by Cai-Nicolas Ziegler through a 4-week crawl on www.bookcrossing.com.\n",
    "\n",
    "A brief overview of the dataset:\n",
    "There are 3 CSV files,\n",
    "-\tUsers.csv: 278,858 rows of users and the 3 columns are User-ID, location and age\n",
    "-\tBooks.csv: 271,379 rows of books and the 7 columns are Book-Title, Book-Author, Year-Of-Publication, Publisher, Image-URL-S, Image-URL-M and Image-URL-L.\n",
    "-\tRatings.csv: 1,149,780 rows of book ratings where the 3 columns are User-ID, ISBN, and ratings\n",
    "\n",
    "Additional, to supplement content-filtering model, we used various webcrawing technique and API to extract book description and category/genre information from Google books and Openlibrary. We then combined the above information in Books.csv such that each book has 4 additional columns named Desription, Category, Openlibrary_Desription and Openlibrary_category.\n",
    "\n",
    "**Exploratory data analysis on the dataset:**\n",
    "\n",
    "\n",
    "**Weaknesses:**\n",
    "**User ratings distribution:**\n",
    "<img src=\"user distribution.png\">\n",
    "From the distribution, we can see that the vast majority of the users have very few ratings. This is an extremely right-skewed distribution with a small minority of the users having most of the ratings (notice the log-scale). The implication is that the recommendations provided may be biased in favour of these more \"vocal\" users. Further, observe the following statistics:\n",
    "\n",
    "Total number of users: 278858\n",
    "Total number of books: 83580 (as tallied in books.csv)\n",
    "Number of users with ratings: 105283\n",
    "Number of books with ratings: 340556\n",
    "Number of users without ratings: 173575 (total_users in users.csv - rated_users in ratings.csv)\n",
    "Number of books without ratings: -256976 (total_books in books.csv - rated_books in ratings.csv)\n",
    "\n",
    "Observations:\n",
    "- There are 173575 users who never rated any books (62% of all users!).\n",
    "- Notice the negative number of books without ratings. This is because there are more books in ratings.csv than the books in books.csv. This is a mistake caused by the original owner who gathered the data. The implication is that there are occasions where we could not retrieve full details of the recommended book because it doesn't exist in books.csv where information such as book title, authour and book description are located. We discovered the problem too late so we had to find other recommended books to fill the top-N recommended book list (Sacrificing relevancy since the list is sorted based on relevancy).\n",
    "\n",
    "Another weakness of the dataset which we overcame is the fact that the original dataset doesn't have book description which content-based model depends on. However, we solved this problem by webscraping book description from online websites.\n",
    "\n",
    "**Strength:**\n",
    "First of all, the dataset is very suited for collaborative-filtering model as we have abundance of data on user ratings. We can feed this data directly into our model without much modifications. One worthy mention of the dataset is the extra context about the users such as their geograpical location and age. This could be useful for context-aware recommendation. Another strength is the sheer size of the dataset. However, due to the limitation of our computation resources, we could not utilize the full dataset, unfortunately.\n",
    "\n",
    "\n",
    "**Subset of the dataset:**\n",
    "We decided to use a subset of the data because the original dataset is simply too big for our computers to handle. For example, during matrix factorization, we had to use compressed sparse row (CSR) to represent our matrix because the user-item matrix is simply too big to store in the memory. The runtime of the matrix factorization algortihm also takes too long for the original dataset. There is also a major practical constraint of webscraping for book description. Because we need to retrieve the description for each book through API or webscraping. To obtain description for all 271,379 books is a very time consuming task as only about 40,000 of books can be scraped in 24 hours. We need to scrape 271,379 * 2 number of descriptions for both Google books and Openlibrary. So we needed abount 14 days to obtain the entire set of descriptions which is not permitted by time. In the end, we decided to use about 30% of the original book dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u70FCgXWlAUj"
   },
   "source": [
    "## Methods:\n",
    "\n",
    "**Collaborative-filtering models:**\n",
    "For the Collaborative-filtering models, we have 2 implementations differing only in the method of matrix factorization.\n",
    "\n",
    "**Stochastic gradient descend (SGD) approach:**\n",
    "Objective: Decompose user-book ratings matrix R into 2 lower-dimensional matrices:\n",
    "- P_mxk (user-latent factor matrix)\n",
    "- Q_nxk (item-latent factor matrix)\n",
    "\n",
    "The goal is to minimize the difference between the actual ratings and the predicted ratings (from dot product of P and Q^T)\n",
    "\n",
    "<img src=\"Gradient2.jpg\">\n",
    "<img src=\"Gradient1.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "7tBn8vkNlAUj",
    "outputId": "e5b6f78d-0a7c-458b-a0f1-c2aae441ce8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the original datasets\n",
    "\"\"\" books = pd.read_csv('combined_books.csv')\n",
    "users = pd.read_csv('Users.csv')\n",
    "ratings = pd.read_csv('Ratings.csv') \"\"\"\n",
    "\n",
    "books = pd.read_csv(\"sample_books.csv\")\n",
    "users = pd.read_csv(\"sample_users.csv\")\n",
    "ratings = pd.read_csv(\"sample_ratings.csv\")\n",
    "\n",
    "\n",
    "# Create mappings from books.csv and users.csv\n",
    "user_id_mapping = {id: idx for idx, id in enumerate(ratings[\"User-ID\"].unique())}\n",
    "book_id_mapping = {id: idx for idx, id in enumerate(ratings[\"ISBN\"].unique())}\n",
    "\n",
    "\n",
    "ratings[\"User-ID\"] = ratings[\"User-ID\"].map(user_id_mapping)\n",
    "ratings[\"ISBN\"] = ratings[\"ISBN\"].map(book_id_mapping)\n",
    "\n",
    "# Create user-item rating matrix\n",
    "n_users = len(user_id_mapping)\n",
    "n_items = len(book_id_mapping)\n",
    "\n",
    "row = ratings[\"User-ID\"].values\n",
    "col = ratings[\"ISBN\"].values\n",
    "data = ratings[\"Book-Rating\"].values\n",
    "R = csr_matrix((data, (row, col)), shape=(n_users, n_items))\n",
    "\n",
    "# user-item matrix\n",
    "print(R.toarray())\n",
    "\n",
    "\n",
    "# Function to implement NMF using gradient descent\n",
    "def nmf(R, k, alpha=0.01, lambda_=0.1, n_iterations=2000):\n",
    "    n_users, n_items = R.shape\n",
    "    P = np.random.rand(n_users, k)\n",
    "    Q = np.random.rand(n_items, k).T\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        # print(iteration)\n",
    "        for u in range(n_users):\n",
    "            for i in range(n_items):\n",
    "                if R[u, i] > 0:\n",
    "                    error = R[u, i] - np.dot(P[u, :], Q[:, i])\n",
    "                    # P[u, :] -= alpha * (2 * error * Q[:, i] +  2 * lambda_ * P[u, :])\n",
    "                    # Q[:, i] -= alpha * (2 * error * P[u, :] +  2 * lambda_ * Q[:, i])\n",
    "\n",
    "                    P[u, :] += alpha * (2 * error * Q[:, i] - 2 * lambda_ * P[u, :])\n",
    "                    Q[:, i] += alpha * (2 * error * P[u, :] - 2 * lambda_ * Q[:, i])\n",
    "\n",
    "    return P, Q.T\n",
    "\n",
    "\n",
    "k_values = 2\n",
    "\n",
    "\n",
    "P, Q = nmf(R, k_values, alpha=0.01, lambda_=0.1, n_iterations=1000)\n",
    "\n",
    "R_pred = np.dot(P, Q.T)\n",
    "R_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gws8V0FBlAUk"
   },
   "source": [
    "<img src=\"Pattern.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzHifUtBlAUk"
   },
   "source": [
    "### Reflection\n",
    "The implementation of matrix factorization used in collaborative filtering\n",
    "\n",
    "In our honest opinion, the current recommendation as it is, is not commercially viable and more work needs to be done to improve the relevancy of the recommendations. The most obvious areas of improvement is the improvement of the  \n",
    "\n",
    "**Furthure improvement:**\n",
    "context-aware recommendation: To furthure improve the relevancy of the collaborative filtering model, a potential area of improvement is the usage of the generalised factorization machine introduced in the context-aware recommendation lecture. We could consider adding additional dimensions to the matrix factorization problem with context variables such as time/days, location and age. By capturing the hidden relationship between users, books and context variables, we believe we could significantly improve the relevancy of the recommendations. In fact, the current dataset does contain context information such as users' location and age. However, due to time constraint, we could not experiment furthure with this idea. From our experience with the traditional matrix factorization problem, we realised the great difficulty of decomposing a matrix as sparse as the user-item matrix, where the matrix is enourmously large (so large that we could not fit the entire matrix in RAM without converting it to CSR) yet the vast majority of the entries are empty. We could only imagine the greater difficulty of adding more dimensions to the problem and thus have an even larger sparse matrix (more accurately, tensors). This problem may be beyond our current capability to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x06poxoslAUk"
   },
   "source": [
    "## Content-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkImR5b8lAUk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "books_df = pd.read_csv(\"combined_books.csv\")\n",
    "ratings_df = pd.read_csv(\"content_Ratings.csv\")\n",
    "users_df = pd.read_csv(\"content_Users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4Ph9OhwlAUk"
   },
   "outputs": [],
   "source": [
    "new_user_data = {\"User-ID\": 279858, \"Location\": \"kensington, nsw, australia\", \"Age\": 38}\n",
    "\n",
    "# Append the new user to the DataFrame\n",
    "users_df = pd.concat([users_df, pd.DataFrame([new_user_data])], ignore_index=True)\n",
    "\n",
    "# List of ISBNs to be rated by the new user\n",
    "isbn_list = [\n",
    "    \"0425176428\",\n",
    "    \"0385418493\",\n",
    "    \"0871137380\",\n",
    "    \"0486265862\",\n",
    "    \"0060973129\",\n",
    "    \"0029087104\",\n",
    "    \"0553256696\",\n",
    "    \"0393038440\",\n",
    "    \"0345308239\",\n",
    "    \"0131337033\",\n",
    "]\n",
    "\n",
    "# Generate random high ratings (6-10)\n",
    "ratings_list = np.random.randint(6, 11, size=len(isbn_list))\n",
    "\n",
    "# Create new user ratings\n",
    "new_user_ratings = [\n",
    "    {\"User-ID\": 279858, \"ISBN\": isbn, \"Book-Rating\": rating}\n",
    "    for isbn, rating in zip(isbn_list, ratings_list)\n",
    "]\n",
    "\n",
    "ratings_df = pd.concat([ratings_df, pd.DataFrame(new_user_ratings)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "BudgRTfDlAUk",
    "outputId": "4c9d917b-b62f-42c2-c207-ba19f185e947"
   },
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "LyJq0w5JlAUk",
    "outputId": "f91eaa2b-7b76-4a1a-9712-54692ba4ad88"
   },
   "outputs": [],
   "source": [
    "missing_values = books_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "efRhMtr9lAUk",
    "outputId": "cdcf63a7-73d4-456f-e656-160637ca732f"
   },
   "outputs": [],
   "source": [
    "# Fill in the Description and openlibrary_Description columns\n",
    "books_df[\"Description\"] = books_df[\"Description\"].fillna(\n",
    "    books_df[\"openlibrary_Description\"]\n",
    ")\n",
    "books_df[\"openlibrary_Description\"] = books_df[\"openlibrary_Description\"].fillna(\n",
    "    books_df[\"Description\"]\n",
    ")\n",
    "\n",
    "# Delete lines where both Description and openlibrary_Description are empty\n",
    "books_df = books_df.dropna(subset=[\"Description\", \"openlibrary_Description\"], how=\"all\")\n",
    "\n",
    "# Fill in the Description and openlibrary_Description columns\n",
    "books_df[\"Categories\"] = books_df[\"Categories\"].fillna(\n",
    "    books_df[\"openlibrary_Categories\"]\n",
    ")\n",
    "books_df[\"openlibrary_Categories\"] = books_df[\"openlibrary_Categories\"].fillna(\n",
    "    books_df[\"Categories\"]\n",
    ")\n",
    "\n",
    "# Delete lines where both Description and openlibrary_Description are empty\n",
    "books_df = books_df.dropna(subset=[\"Categories\", \"openlibrary_Categories\"], how=\"all\")\n",
    "\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "GiloBRozlAUk",
    "outputId": "4dc8b34d-8cae-4a66-f1a0-4529917a76cf"
   },
   "outputs": [],
   "source": [
    "missing_values = books_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "1ytfFcPMlAUk",
    "outputId": "ded0d1f7-007b-4284-e983-102e3a64f6a6"
   },
   "outputs": [],
   "source": [
    "books_df.loc[books_df[\"Year-Of-Publication\"] == \"DK Publishing Inc\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "ijWZ7PhjlAUk",
    "outputId": "4c141743-da49-479f-f3bd-59de3ca626a1"
   },
   "outputs": [],
   "source": [
    "# ISBN '0789466953'\n",
    "books_df.loc[books_df[\"ISBN\"] == \"0789466953\", \"Year-Of-Publication\"] = 2000\n",
    "books_df.loc[books_df[\"ISBN\"] == \"0789466953\", \"Book-Author\"] = \"James Buckley\"\n",
    "books_df.loc[books_df[\"ISBN\"] == \"0789466953\", \"Publisher\"] = \"DK Publishing Inc\"\n",
    "books_df.loc[books_df[\"ISBN\"] == \"0789466953\", \"Book-Title\"] = (\n",
    "    \"DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\"\n",
    ")\n",
    "\n",
    "# ISBN '078946697X'\n",
    "books_df.loc[books_df[\"ISBN\"] == \"078946697X\", \"Year-Of-Publication\"] = 2000\n",
    "books_df.loc[books_df[\"ISBN\"] == \"078946697X\", \"Book-Author\"] = \"Michael Teitelbaum\"\n",
    "books_df.loc[books_df[\"ISBN\"] == \"078946697X\", \"Publisher\"] = \"DK Publishing Inc\"\n",
    "books_df.loc[books_df[\"ISBN\"] == \"078946697X\", \"Book-Title\"] = (\n",
    "    \"DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\"\n",
    ")\n",
    "\n",
    "# rechecking\n",
    "books_df.loc[(books_df[\"ISBN\"] == \"0789466953\") | (books_df[\"ISBN\"] == \"078946697X\"), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "s9QZzf4UlAUl",
    "outputId": "aa89816f-03cf-4278-dd59-fa3504dad3ee"
   },
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7Ru436i-lAUl",
    "outputId": "2a3447ce-ce49-4bda-9dd0-ee32654050a5"
   },
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure merged_df right\n",
    "merged_df = pd.merge(books_df, ratings_df, on=\"ISBN\")\n",
    "\n",
    "# final_df\n",
    "final_df = pd.merge(merged_df, users_df, on=\"User-ID\")\n",
    "\n",
    "# Function to normalize book titles by removing special characters and converting to lowercase\n",
    "def normalize_title(title):\n",
    "    return re.sub(r'[^\\w\\s]', '', title.lower())\n",
    "\n",
    "# Apply normalization to the Book-Title and ISBN columns\n",
    "final_df[\"Normalized-Title\"] = final_df[\"Book-Title\"].apply(normalize_title)\n",
    "final_df[\"Normalized-ISBN\"] = final_df[\"ISBN\"].apply(lambda x: re.sub(r'[^\\dX]', '', x.upper()))\n",
    "\n",
    "# Drop duplicate entries based on the normalized columns\n",
    "final_df = final_df.drop_duplicates(subset=[\"Normalized-Title\", \"Normalized-ISBN\"])\n",
    "\n",
    "# Drop the normalized columns after deduplication (optional, based on your needs)\n",
    "final_df = final_df.drop(columns=[\"Normalized-Title\", \"Normalized-ISBN\"])\n",
    "\n",
    "# Recheck the final DataFrame\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "_MM4HufPlAUl",
    "outputId": "5dc505ad-3186-4a4f-fe92-9ec2c0eb5fb5"
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the final merged DataFrame\n",
    "missing_values = final_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsu4WLbJlAUl",
    "outputId": "c4c63da9-e814-4944-e438-cdad53651b43"
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "aiNuCFvslAUl",
    "outputId": "0c76e694-2edd-45f9-e5b5-b695f9890f49"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the global drawing style and font size\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams.update({\"font.size\": 12})\n",
    "ratings = ratings_df[ratings_df.ISBN.isin(books_df.ISBN)]\n",
    "ratings_explicit = ratings[ratings[\"Book-Rating\"] != 0]\n",
    "ratings_implicit = ratings[ratings[\"Book-Rating\"] == 0]\n",
    "# Plot the distribution of book ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=ratings_explicit, x=\"Book-Rating\", palette=\"rocket_r\")\n",
    "plt.title(\"Distribution of Explicit Book Ratings\", fontsize=16)\n",
    "plt.xlabel(\"Book Rating\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "IozaiIYXlAUl",
    "outputId": "74eb0e99-8d6b-4a3b-bbf4-3c0e0ce67e6b"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the number of user ratings\n",
    "user_rating_counts = ratings_df.groupby(\"User-ID\")[\"ISBN\"].count()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    user_rating_counts, kde=True, log_scale=(True, False), bins=30, color=\"teal\"\n",
    ")\n",
    "plt.title(\"Distribution of Total Ratings per User\", fontsize=16)\n",
    "plt.xlabel(\"Number of Ratings\", fontsize=14)\n",
    "plt.ylabel(\"Number of Users\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "ZDaKv4F3lAUl",
    "outputId": "f53c3320-cab6-48a2-e567-d3d13f95c55e"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of user ages\n",
    "df_plot = users_df[users_df[\"Age\"] <= 100]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_plot[\"Age\"], kde=True, color=\"#4169E1\")\n",
    "plt.title(\"Distribution of User Ages\", fontsize=16)\n",
    "plt.xlabel(\"Age\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "1pPPCst7lAUl",
    "outputId": "fc944928-8b84-4176-df1b-ce40e2ce2ae6"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the year the book was published\n",
    "books_df[\"Year-Of-Publication\"] = pd.to_numeric(\n",
    "    books_df[\"Year-Of-Publication\"], errors=\"coerce\"\n",
    ")\n",
    "df_plot2 = books_df[\n",
    "    (books_df[\"Year-Of-Publication\"] > 1950) & (books_df[\"Year-Of-Publication\"] <= 2016)\n",
    "]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_plot2[\"Year-Of-Publication\"], kde=True, color=\"#FFA07A\")\n",
    "plt.title(\"Distribution of Year of Publication\", fontsize=16)\n",
    "plt.xlabel(\"Year of Publication\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "EwUmXX-wlAUl",
    "outputId": "30df4463-7c4f-4b16-843a-41d736aa44b8"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the most popular book Categories (Top 10 Categories)\n",
    "top_categories = books_df[\"Categories\"].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_categories.values, y=top_categories.index, palette=\"magma\")\n",
    "plt.title(\"Top 10 Book Categories\", fontsize=16)\n",
    "plt.xlabel(\"Number of Books\", fontsize=14)\n",
    "plt.ylabel(\"Categories\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "-JdXTjTOlAUl",
    "outputId": "35f641ac-065e-424b-c885-348e09f9b574"
   },
   "outputs": [],
   "source": [
    "# Top 10 Authors by Number of Ratings\n",
    "# Merge books_df and ratings_df and count the number of ratings per author\n",
    "data = pd.merge(books_df, ratings_df, on=\"ISBN\")[\n",
    "    [\"Book-Author\", \"Book-Rating\", \"Book-Title\", \"ISBN\"]\n",
    "]\n",
    "\n",
    "# Group and aggregate the mean and quantity of 'Book-Rating' by 'Book-Author'\n",
    "data = data.groupby(\"Book-Author\").agg({\"Book-Rating\": [\"mean\", \"count\"]}).reset_index()\n",
    "\n",
    "data.columns = [\"Book-Author\", \"mean\", \"count\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_rated_authors = data.sort_values(\"count\", ascending=False).head(10)\n",
    "sns.barplot(x=\"count\", y=\"Book-Author\", data=top_rated_authors, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Authors by Number of Ratings\", fontsize=16)\n",
    "plt.xlabel(\"Number of Ratings\", fontsize=14)\n",
    "plt.ylabel(\"Author\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "JHpLknbKlAUm",
    "outputId": "8d3ef5e9-df58-44ad-d483-ba5b58740560"
   },
   "outputs": [],
   "source": [
    "# Aggregate score data for books\n",
    "data = ratings_df.groupby(\"ISBN\").agg([\"mean\", \"count\"])[\"Book-Rating\"].reset_index()\n",
    "\n",
    "# Merge rating data with book data to get author information\n",
    "data = pd.merge(data, books_df[[\"ISBN\", \"Book-Author\"]], on=\"ISBN\")\n",
    "\n",
    "# Calculate the weighted score\n",
    "m = data[\"count\"].quantile(0.90)  \n",
    "C = data[\"mean\"].mean()  \n",
    "data = data[data[\"count\"] > m]\n",
    "\n",
    "# Weighted scoring formula\n",
    "data[\"weighted rating\"] = (data[\"count\"] / (data[\"count\"] + m)) * data[\"mean\"] + (\n",
    "    m / (m + data[\"count\"])\n",
    ") * C\n",
    "\n",
    "# Weighted Rating vs. Number of Ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x=\"count\",\n",
    "    y=\"weighted rating\",\n",
    "    data=data,\n",
    "    hue=\"mean\",\n",
    "    size=\"mean\",\n",
    "    palette=\"cool\",\n",
    "    sizes=(20, 200),\n",
    "    legend=False,\n",
    ")\n",
    "plt.title(\"Weighted Rating vs. Number of Ratings\", fontsize=16)\n",
    "plt.xlabel(\"Number of Ratings\", fontsize=14)\n",
    "plt.ylabel(\"Weighted Rating\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_author_name(name):\n",
    "    # Remove special characters, convert to lowercase, and remove spaces\n",
    "    name = re.sub(r'[^\\w\\s]', '', name.lower())\n",
    "    return name\n",
    "\n",
    "\n",
    "# Load the combined books dataset\n",
    "books_df = pd.read_csv(\"combined_books.csv\")\n",
    "\n",
    "# Create a new column for normalized author names\n",
    "books_df[\"Normalized-Author\"] = books_df[\"Book-Author\"].apply(normalize_author_name)\n",
    "\n",
    "# Drop duplicate books based on 'Normalized-Author' and 'Book-Title'\n",
    "books_df = books_df.drop_duplicates([\"Normalized-Author\", \"Book-Title\"])\n",
    "\n",
    "# Merge books and ratings dataframes on 'ISBN'\n",
    "ratings_df = pd.read_csv(\"content_Ratings.csv\")\n",
    "data = pd.merge(books_df, ratings_df, on=\"ISBN\")[\n",
    "    [\"Book-Author\", \"Normalized-Author\", \"Book-Rating\", \"Book-Title\", \"ISBN\"]\n",
    "]\n",
    "\n",
    "# Group by 'Normalized-Author' and aggregate mean and count of 'Book-Rating'\n",
    "data = (\n",
    "    data.groupby(\"Normalized-Author\")\n",
    "    .agg({\"Book-Rating\": [\"mean\", \"count\"], \"Book-Author\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.columns = [\"Normalized-Author\", \"mean\", \"count\", \"Book-Author\"]\n",
    "\n",
    "# Determine the threshold for number of votes\n",
    "m = data[\"count\"].quantile(0.99)\n",
    "data = data[data[\"count\"] > m]\n",
    "\n",
    "print(\"m =\", m)\n",
    "print(data.shape)\n",
    "\n",
    "# Calculate weighted rating\n",
    "R = data[\"mean\"]  # Average rating for the author\n",
    "v = data[\"count\"]  # Number of votes for the author\n",
    "C = data[\"mean\"].mean()  # Mean vote across all authors\n",
    "data[\"weighted rating\"] = (v / (v + m)) * R + (m / (v + m)) * C\n",
    "\n",
    "# Sort by weighted rating and reset index\n",
    "data = data.sort_values(\"weighted rating\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20 highest rated authors\n",
    "print(data[[\"Book-Author\", \"mean\", \"count\", \"weighted rating\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "id": "HbDt9IirlAUm",
    "outputId": "b5259d79-440c-4c61-d36c-5fc10bef662f"
   },
   "outputs": [],
   "source": [
    "# Top 20 Highest Rated Books\n",
    "data = ratings_df.groupby(\"ISBN\").agg([\"mean\", \"count\"])[\"Book-Rating\"].reset_index()\n",
    "\n",
    "\n",
    "m = data[\"count\"].quantile(0.99)  # Keep only books that are rated more than this value\n",
    "data = data[data[\"count\"] > m]\n",
    "print(\"m =\", m)\n",
    "print(data.shape)\n",
    "R = data[\"mean\"]  # average for the book rating\n",
    "v = data[\"count\"]  # number of votes for the book = (votes)\n",
    "C = data[\"mean\"].mean()  # mean vote across all books\n",
    "data[\"weighted rating\"] = (v / (v + m)) * R + (m / (v + m)) * C\n",
    "data = data.sort_values(\"weighted rating\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "data = (\n",
    "    pd.merge(data, books_df, on=\"ISBN\")[\n",
    "        [\n",
    "            \"Book-Title\",\n",
    "            \"Book-Author\",\n",
    "            \"mean\",\n",
    "            \"count\",\n",
    "            \"weighted rating\",\n",
    "            \"Year-Of-Publication\",\n",
    "        ]\n",
    "    ]\n",
    "    .drop_duplicates(\"Book-Title\")\n",
    "    .iloc[:20]\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNtQ15kZlAUm"
   },
   "source": [
    "### Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "O8JwhwANlAUm",
    "outputId": "23c11f79-8b3c-422e-fc9a-c526fdb646b9"
   },
   "outputs": [],
   "source": [
    "missing_values = final_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5x9dSYKlAUm"
   },
   "outputs": [],
   "source": [
    "final_df[\"Book-Author\"].fillna(final_df[\"Book-Author\"].mode()[0], inplace=True)\n",
    "final_df[\"Publisher\"].fillna(final_df[\"Publisher\"].mode()[0], inplace=True)\n",
    "\n",
    "# Handle missing values in 'Age' column with median\n",
    "final_df[\"Age\"].fillna(final_df[\"Age\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "a53MnevglAUm",
    "outputId": "726a1390-05cf-4b67-d3b3-711d22f14ca1"
   },
   "outputs": [],
   "source": [
    "missing_values = final_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUHiq10ZlAUm",
    "outputId": "6c6df875-45da-47cf-be04-796527514f2b"
   },
   "outputs": [],
   "source": [
    "# Converts the data types of the three fields in the final_df data box to integers\n",
    "final_df[\"Age\"] = final_df[\"Age\"].astype(int)\n",
    "final_df[\"Book-Rating\"] = final_df[\"Book-Rating\"].astype(int)\n",
    "final_df[\"User-ID\"] = final_df[\"User-ID\"].astype(int)\n",
    "\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTgBSIXDlAUm",
    "outputId": "c5e59cd6-c75e-4fdf-f2d2-237c3b478580"
   },
   "outputs": [],
   "source": [
    "duplicate_count = final_df.duplicated().sum()\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JRwC_R8RlAUn",
    "outputId": "cff057dd-c24a-416d-c29d-d9a47c2e2474"
   },
   "outputs": [],
   "source": [
    "# Remove the data whose rating is 0\n",
    "zero_rating_books_df = final_df[final_df[\"Book-Rating\"] == 0]\n",
    "zero_rating_books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtyONkf9lAUt"
   },
   "outputs": [],
   "source": [
    "final_df = final_df[final_df[\"Book-Rating\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "9AunpuZklAUt",
    "outputId": "1d2768cc-a3d7-482f-f725-d6461cf73fbe"
   },
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "df3 = final_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "\n",
    "df3[\"Rating_Category\"] = pd.cut(\n",
    "    df3[\"Book-Rating\"], bins=[0, 3, 7, 10], labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "\n",
    "# Plotting bar plot dengan seaborn\n",
    "plt.figure(figsize=(18, 20))\n",
    "sns.barplot(\n",
    "    data=df3,\n",
    "    x=\"Book-Rating\",\n",
    "    y=\"Book-Title\",\n",
    "    hue=\"Rating_Category\",\n",
    "    palette={\"Low\": \"red\", \"Medium\": \"orange\", \"High\": \"green\"},\n",
    ")\n",
    "plt.title(\"Average Rating by Title of Book(new)\")\n",
    "plt.xlabel(\"Average Rating\")\n",
    "plt.ylabel(\"Title of Book\")\n",
    "plt.legend(title=\"Rating Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "pQSsU0FMlAUt",
    "outputId": "cc14ff8b-db23-40a8-b093-a2effd71b2b3"
   },
   "outputs": [],
   "source": [
    "# Data statistics description\n",
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFlzu1T5lAUu"
   },
   "source": [
    "First identify and mark outliers (greater than 2022 or equal to 0) in the 'Year-Of-Publication' field as missing values.\n",
    "The average of the 'Year-Of-Publication' field in books_df is then used to populate the missing value of that field in final_df.\n",
    "Finally, the populated value is converted to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R-nfcmslAUu"
   },
   "outputs": [],
   "source": [
    "# Handle outliers in the 'Year-Of-Publication' column\n",
    "import numpy as np\n",
    "\n",
    "final_df.loc[\n",
    "    (final_df[\"Year-Of-Publication\"] > 2022) | (final_df[\"Year-Of-Publication\"] == 0),\n",
    "    \"Year-Of-Publication\",\n",
    "] = np.nan\n",
    "final_df.loc[:, \"Year-Of-Publication\"] = (\n",
    "    final_df[\"Year-Of-Publication\"]\n",
    "    .fillna(round(books_df[\"Year-Of-Publication\"].mean()))\n",
    "    .astype(np.int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvcC5nu6lAUu"
   },
   "source": [
    "Then identify and mark outliers in the 'Age' field (greater than 90 years old or less than 5 years old) as missing values.\n",
    "The average value of the 'Age' field in users_df is then used to populate the missing value of that field in final_df.\n",
    "Finally, the data types of the 'Age' and 'Year-Of-Publication' fields are converted to 32-bit integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eco8rnT_lAUu"
   },
   "outputs": [],
   "source": [
    "final_df.loc[(final_df.Age > 90) | (final_df.Age < 5), \"Age\"] = np.nan\n",
    "\n",
    "# replacing NaNs with mean\n",
    "final_df.Age = final_df.Age.fillna(users_df.Age.mean())\n",
    "\n",
    "# setting the data type as int\n",
    "final_df.Age = final_df.Age.astype(np.int32)\n",
    "final_df[\"Year-Of-Publication\"] = final_df[\"Year-Of-Publication\"].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "EpHhYCzElAUu",
    "outputId": "d98e4a34-3f73-4a7a-b84e-1357e9be64c8"
   },
   "outputs": [],
   "source": [
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVts9peQlAUu"
   },
   "source": [
    "Data preparation:\n",
    "\n",
    "First, the 'Book-Title', 'Book-Author', 'Description', and 'Categories' columns in books_df are concatenated into a text corpus.\n",
    "Using Gensim's Word2Vec model, a word vector model word2vec_model_recommender is obtained after training in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwsCJ35hlAUu"
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus = (\n",
    "    (\n",
    "        books_df[\"Book-Title\"].astype(str)\n",
    "        + \" \"\n",
    "        + books_df[\"Book-Author\"].astype(str)\n",
    "        + \" \"\n",
    "        + books_df[\"Description\"].astype(str)\n",
    "        + \" \"\n",
    "        + books_df[\"Categories\"].astype(str)\n",
    "        + \" \"\n",
    "        + books_df[\"openlibrary_Description\"].astype(str)\n",
    "        + \" \"\n",
    "        + books_df[\"openlibrary_Categories\"].astype(str)\n",
    "    )\n",
    "    .apply(str.split)\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpxSuCVdlAUu"
   },
   "source": [
    "**Recommended algorithm Word2Vec:**\n",
    "- The recommend function is the core of the recommendation algorithm.\n",
    "- The input parameters are the user ID user_id, the complete book data, and the trained Word2Vec model word2vec_model.\n",
    "- First get the user's favorite book information, including title, author, category, and description.\n",
    "- This information is spliced into a text and divided into words to obtain an average word vector avg_vector.\n",
    "- Then the average word vector of each book is calculated, and the cosine similarity is calculated with the average word vector of the  user, and the similarity score is obtained.\n",
    "- According to the similarity score, the top 10 books with the highest similarity and scores higher than 5 are selected as the recommended results.\n",
    "- Finally, the recommendation result is returned in DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DXFFihtLlAUu",
    "outputId": "82ed843c-7cea-4bed-ffa1-bccf16b6fdad"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model_recommender = Word2Vec(\n",
    "    sentences=corpus, vector_size=500, window=5, min_count=5, sg=2\n",
    ")\n",
    "\n",
    "\n",
    "def recommend(user_id, data, word2vec_model):\n",
    "    # Get user preferences\n",
    "    user_preferences = data[data[\"User-ID\"] == user_id]\n",
    "    if user_preferences.empty:\n",
    "        return None\n",
    "\n",
    "    # Get information about the user's favorite books\n",
    "    liked_books = user_preferences[\"Book-Title\"].tolist()\n",
    "    liked_ISBN = user_preferences[\"ISBN\"].tolist()\n",
    "    liked_authors = user_preferences[\"Book-Author\"].tolist()\n",
    "    liked_genres = user_preferences[\"Categories\"].tolist()\n",
    "    liked_description = user_preferences[\"Description\"].tolist()\n",
    "    liked_openlibrary_description = user_preferences[\"openlibrary_Description\"].tolist()\n",
    "    liked_openlibrary_categories = user_preferences[\"openlibrary_Categories\"].tolist()\n",
    "\n",
    "    # Merge user preference information\n",
    "    text = \" \".join(\n",
    "        liked_ISBN\n",
    "        + liked_books\n",
    "        + liked_authors\n",
    "        + liked_genres\n",
    "        + liked_description\n",
    "        + liked_openlibrary_description\n",
    "        + liked_openlibrary_categories\n",
    "    )\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Get text vector\n",
    "    vectors = [\n",
    "        word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv\n",
    "    ]\n",
    "    if len(vectors) == 0:\n",
    "        return None\n",
    "    avg_vector = sum(vectors) / len(vectors)\n",
    "\n",
    "    # Calculate the similarity to each book\n",
    "    similarities = []\n",
    "    recommended_titles = set()\n",
    "    for idx, row in data.iterrows():\n",
    "        # Skip books that the user has already rated\n",
    "        if row[\"ISBN\"] in liked_ISBN:\n",
    "            continue\n",
    "\n",
    "        row_text = \" \".join(\n",
    "            [\n",
    "                str(row[col])\n",
    "                for col in [\n",
    "                    \"Book-Title\",\n",
    "                    \"Book-Author\",\n",
    "                    \"Categories\",\n",
    "                    \"Description\",\n",
    "                    \"openlibrary_Description\",\n",
    "                    \"openlibrary_Categories\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "        row_tokens = row_text.split()\n",
    "        row_vectors = [\n",
    "            word2vec_model.wv[token]\n",
    "            for token in row_tokens\n",
    "            if token in word2vec_model.wv\n",
    "        ]\n",
    "        if len(row_vectors) > 0:\n",
    "            row_avg_vector = sum(row_vectors) / len(row_vectors)\n",
    "            similarity = cosine_similarity([avg_vector], [row_avg_vector])[0][0]\n",
    "            similarities.append((row, similarity))\n",
    "\n",
    "    # Rank the similarity and select the top 10 recommendations\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    recommendations = []\n",
    "    for book, sim in similarities:\n",
    "        recommendations.append(book.to_dict())\n",
    "        recommended_titles.add(book[\"Book-Title\"])\n",
    "        if len(recommendations) >= 10:\n",
    "            break\n",
    "\n",
    "    # Convert the recommendation result to a DataFrame\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "    return recommendations_df\n",
    "\n",
    "\n",
    "# Example of using the function\n",
    "user_id = 279858\n",
    "recommendations = recommend(user_id, final_df, word2vec_model_recommender)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4mhyjZalAUu"
   },
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Splicing the 'Book Title', 'Book Author', 'Description' and 'Categories' columns in' final_df 'into a large text corpus.\n",
    "Use 'TfidfVectorizer' to extract TF-IDF features from 'corpus' and generate tfidf_matrix.\n",
    "\n",
    "**Recommendation Algorithm**:\n",
    "- The 'recommend' function accepts the user ID and the entire dataset 'final_df' as input.\n",
    "- First, get the user's favorite book list 'user_preferences'.\n",
    "- Then, generate the user preference vector user_vector according to user_preferences.\n",
    "- Next, traverse the entire data set, calculating the cosine similarity between each book and the user's preference vector, and store the results in the similarities list.\n",
    "- Sorts the similarities list in descending order of similarity.\n",
    "- From the similarities list after sorting, select books that the first 10 users are not familiar with, score more than 5 points, and do not repeat as the recommendation results.\n",
    "- Finally, the recommendation result is returned in DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = (books_df['Book-Title'].astype(str) + ' ' + \n",
    "          books_df['Book-Author'].astype(str) + ' ' +\n",
    "          books_df['Description'].astype(str) + ' ' +\n",
    "          books_df['Categories'].astype(str)).tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "def recommend(user_id, data):\n",
    "    # Get user preferences\n",
    "    user_preferences = final_df.loc[final_df['User-ID'] == user_id, 'Book-Title'].tolist()\n",
    "    \n",
    "    # Create a user preference vector\n",
    "    user_vector = tfidf_vectorizer.transform([' '.join(user_preferences)])\n",
    "    \n",
    "    # Calculate how similar each book is to user preferences\n",
    "    similarities = []\n",
    "    for idx, row in data.iterrows():\n",
    "        row_text = ' '.join([str(row[col]) for col in data.columns])\n",
    "        row_vector = tfidf_vectorizer.transform([row_text])\n",
    "        similarity = cosine_similarity(user_vector, row_vector)[0][0]\n",
    "        similarities.append((row, similarity))\n",
    "    \n",
    "    # Sort in descending order of similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 recommendations\n",
    "    recommendations = []\n",
    "    recommended_titles = set()\n",
    "    for book, sim in similarities:\n",
    "        if book['Book-Title'] not in user_preferences and book['Book-Rating'] > 5 and book['Book-Title'] not in recommended_titles:\n",
    "            recommendations.append(book.to_dict())\n",
    "            recommended_titles.add(book['Book-Title'])\n",
    "        if len(recommendations) >= 10:\n",
    "            break\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "    return recommendations_df\n",
    "\n",
    "user_id = 279858\n",
    "recommendations = recommend(user_id, final_df)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C79H8vz1lAUv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def evaluate_recommendations(data, word2vec_model, top_n=10):\n",
    "    user_ids = data['User-ID'].unique()\n",
    "    \n",
    "    all_recall = []\n",
    "    all_precision = []\n",
    "    hit_rate = []\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        recommendations = recommend(user_id, data, word2vec_model)\n",
    "        if recommendations is None or recommendations.empty:\n",
    "            continue\n",
    "        \n",
    "        user_preferences = data[data['User-ID'] == user_id]\n",
    "        liked_books = user_preferences['Book-Title'].tolist()\n",
    "        \n",
    "        recommended_books = recommendations['Book-Title'].tolist()\n",
    "        true_positives = [1 if book in liked_books else 0 for book in recommended_books[:top_n]]\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = sum(true_positives) / top_n\n",
    "        recall = sum(true_positives) / len(liked_books)\n",
    "        \n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        \n",
    "        # Calculate hit rate (whether there is at least one correct recommendation)\n",
    "        if sum(true_positives) > 0:\n",
    "            hit_rate.append(1)\n",
    "        else:\n",
    "            hit_rate.append(0)\n",
    "    \n",
    "    # Calculate the average recall, precision, and hit rate across all users\n",
    "    average_recall = np.mean(all_recall)\n",
    "    average_precision = np.mean(all_precision)\n",
    "    hit_rate = np.mean(hit_rate)\n",
    "    \n",
    "    return average_recall, average_precision, hit_rate\n",
    "\n",
    "# Example of using the evaluation function\n",
    "average_recall, average_precision, hit_rate = evaluate_recommendations(final_df, word2vec_model_recommender)\n",
    "print(f'Average Recall: {average_recall:.2f}')\n",
    "print(f'Average Precision: {average_precision:.2f}')\n",
    "print(f'Hit Rate: {hit_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLyx1Ux9lAUv"
   },
   "source": [
    "TP = 7\n",
    "FP = 8\n",
    "TN = impossible\n",
    "FN = impossible\n",
    "Precision = TP/(TP+FP) = 7/15 = 0.47\n",
    "Recall = TP/(TP+FN)\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "Accuracy=(TP+TN)/(whole dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have users 'interaction' with books - theirs ratings, we can use collaborative filtering using these interactions, based on the idea that users who have agreed in the past will agree in the future.\n",
    "\n",
    "In our project, Alternating Least Squares (ALS) algorithm is used to identify the patterns in both users and books. ALS can factorize the large user-item interaction matrix into two lower-dimensional matrices that capture the latent factors of users and items. The final goal of ALS is to minimize the difference between the actual ratings and the predicted ones derived from the latent factors.\n",
    "\n",
    "\n",
    "1. Data preparation \n",
    "\n",
    "First, the user and book IDs are mapped to a range of index, which is essential for creating a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('./content_Ratings.csv')\n",
    "users = pd.read_csv('./content_Users.csv')\n",
    "dtype_spec = {\n",
    "    'ISBN': str,\n",
    "    'Book-Title': str,\n",
    "    'Book-Author': str,\n",
    "    'Year-Of-Publication': str,\n",
    "    'Publisher': str,\n",
    "    'Image-URL-S': str,\n",
    "    'Image-URL-M': str,\n",
    "    'Image-URL-L': str,\n",
    "    'Description': str,\n",
    "    'Categories': str\n",
    "}\n",
    "books = pd.read_csv('./combined_books.csv', dtype=dtype_spec, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_head = ratings.head()\n",
    "books_head = books.head()\n",
    "users_head = users.head()\n",
    "\n",
    "ratings_head, books_head, users_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_users(users):\n",
    "    users['Age'].fillna(users['Age'].median(), inplace=True)\n",
    "    users['Age'] = users['Age'].astype(int)\n",
    "    return users\n",
    "\n",
    "def preprocess_books(books):\n",
    "    books.fillna('', inplace=True)\n",
    "    books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce').fillna(0).astype(int)\n",
    "    return books\n",
    "users = preprocess_users(users)\n",
    "books = preprocess_books(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train-Test Split\n",
    "\n",
    "The data is split into training and test sets. To ensure that all users and books are represented in the training set, any users or books that are only present in the test set are added back to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "user_id_mapping = {id: idx for idx, id in enumerate(ratings['User-ID'].unique())}\n",
    "book_id_mapping = {id: idx for idx, id in enumerate(ratings['ISBN'].unique())}\n",
    "\n",
    "ratings['User-ID'] = ratings['User-ID'].map(user_id_mapping)\n",
    "ratings['ISBN'] = ratings['ISBN'].map(book_id_mapping)\n",
    "ratings.dropna(subset=['Book-Rating'], inplace=True)\n",
    "all_user_ids = ratings['User-ID'].unique()\n",
    "all_book_ids = ratings['ISBN'].unique()\n",
    "\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "train_user_ids = set(train_data['User-ID'])\n",
    "train_book_ids = set(train_data['ISBN'])\n",
    "missing_users = set(all_user_ids) - train_user_ids\n",
    "missing_books = set(all_book_ids) - train_book_ids\n",
    "\n",
    "missing_data = ratings[ratings['User-ID'].isin(missing_users) | ratings['ISBN'].isin(missing_books)]\n",
    "train_data = pd.concat([train_data, missing_data]).drop_duplicates()\n",
    "\n",
    "n_users = ratings['User-ID'].nunique()\n",
    "n_items = ratings['ISBN'].nunique()\n",
    "train_matrix = csr_matrix((train_data['Book-Rating'], (train_data['User-ID'], train_data['ISBN'])), shape=(n_users, n_items))\n",
    "test_matrix = csr_matrix((test_data['Book-Rating'], (test_data['User-ID'], test_data['ISBN'])), shape=(n_users, n_items))\n",
    "print(\"-------matrix finished---------\")\n",
    "\n",
    "if np.any(np.isnan(train_matrix.data)):\n",
    "    print(\"NaN values found in training matrix\")\n",
    "else:\n",
    "    print(\"No NaN values in training matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create Sparse Matrices\n",
    "\n",
    "Sparse matrices for the training and test data are created. These matrices are used by the ALS to learn the latent factors. Here we also added a test to judge whether there is null values in the training matrix, in case during matric factorization there is error.\n",
    "\n",
    "4. Train the ALS Model\n",
    "\n",
    "An ALS model is initialized and trained using the training data. The model learns latent factors for users and books. Here we set the iteration to be 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=50, use_gpu=False, calculate_training_loss=True)\n",
    "\n",
    "als_model.fit(train_matrix.T, show_progress=True)\n",
    "print(\"-----------------fitting finished-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate the Model\n",
    "\n",
    "The model is evaluated using the test data by calculating the Root Mean Square Error (RMSE) between the predicted ratings and the actual ratings in the test set.\n",
    "\n",
    "We first extract non-zero entries, then iterate over Non-Zero User-Item pairs to calculate predictions, during this we also check index bounds. After the predictions, we first handle empty predictions, If no predictions were made when the predictions list is empty, the function returns infinity, which is a safeguard to indicate that the model could not make any predictions. Finally RMSE is calculated.\n",
    "\n",
    "In our experiment, Test RMSE is: 7.844746002414569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_matrix, als_model):\n",
    "    test_user_items = test_matrix.nonzero()\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    for user, item in zip(test_user_items[0], test_user_items[1]):\n",
    "        if user < als_model.user_factors.shape[0] and item < als_model.item_factors.shape[0]:\n",
    "            prediction = als_model.user_factors[user, :].dot(als_model.item_factors[item, :].T)\n",
    "            predictions.append(prediction)\n",
    "            ground_truth.append(test_matrix[user, item])\n",
    "    if len(predictions) == 0:\n",
    "        return float('inf')  \n",
    "    return np.sqrt(mean_squared_error(ground_truth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluate_model(train_matrix, als_model)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generate Recommendations\n",
    "\n",
    "Since the recommendation is based on user's past action, after checking whether user is in our mapping, we retrieve the ratings given by the user from the training matrix, then use it by the recommend method to filter out books that the user has already rated. The recommendation is based on Implicit library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_als(user_id, num_recommendations=5):\n",
    "    if user_id not in user_id_mapping:\n",
    "        return [] \n",
    "    user_index = user_id_mapping[user_id]\n",
    "    user_ratings = train_matrix[user_index]\n",
    "    recommended_books = als_model.recommend(user_index, user_ratings, N=num_recommendations, filter_already_liked_items=True)\n",
    "    # recommended_book_ids = [list(book_id_mapping.keys())[list(book_id_mapping.values()).index(i)] for i, _ in recommended_books]\n",
    "    # return books[books['ISBN'].isin(recommended_book_ids)]\n",
    "    return recommended_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Conclusion for ALS:\n",
    "\n",
    "By using ALS, we managed to discover hidden patterns and relationships in the rating data by learning latent factors for both users and books. Also, we managed to reduce the high-dimensional user-item matrix into lower-dimensional ones. The RMSE is relatively low, and we show it effeciency in predicting unknown interactions and generate recommendations for users. Also, we can use it to compute similarities between users and items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation based on embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to use embedding to calculate similarities of books and users, thus generating recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is similar:\n",
    "\n",
    "1. Handling Missing Values:\n",
    "\n",
    "Fill missing ages with the median age.\n",
    "Convert 'Year-Of-Publication' to numeric, filling missing values with the median year.\n",
    "\n",
    "2. Encoding Categorical Variables:\n",
    "\n",
    "Convert 'Location', 'Book-Author', and 'Publisher' to categorical codes.\n",
    "\n",
    "3. Normalizing Numerical Features:\n",
    "\n",
    "Standardize 'Age' and 'Year-Of-Publication' using StandardScaler to ensure these features are on a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_mapping = {id: idx for idx, id in enumerate(ratings['User-ID'].unique())}\n",
    "book_id_mapping = {id: idx for idx, id in enumerate(ratings['ISBN'].unique())}\n",
    "\n",
    "ratings['User-ID'] = ratings['User-ID'].map(user_id_mapping)\n",
    "ratings['ISBN'] = ratings['ISBN'].astype(str)\n",
    "books['ISBN'] = books['ISBN'].astype(str)\n",
    "ratings['ISBN'] = ratings['ISBN'].map(book_id_mapping)\n",
    "books['ISBN'] = books['ISBN'].map(book_id_mapping)\n",
    "ratings.dropna(subset=['Book-Rating'], inplace=True)\n",
    "# ratings = ratings[ratings['ISBN'].isin(books['ISBN']) & ratings['User-ID'].isin(users['User-ID'])]\n",
    "\n",
    "all_user_ids = ratings['User-ID'].unique()\n",
    "all_book_ids = ratings['ISBN'].unique()\n",
    "\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "train_user_ids = set(train_data['User-ID'])\n",
    "train_book_ids = set(train_data['ISBN'])\n",
    "missing_users = set(all_user_ids) - train_user_ids\n",
    "missing_books = set(all_book_ids) - train_book_ids\n",
    "\n",
    "missing_data = ratings[ratings['User-ID'].isin(missing_users) | ratings['ISBN'].isin(missing_books)]\n",
    "train_data = pd.concat([train_data, missing_data]).drop_duplicates()\n",
    "\n",
    "n_users = ratings['User-ID'].nunique()\n",
    "n_items = ratings['ISBN'].nunique()\n",
    "train_matrix = csr_matrix((train_data['Book-Rating'], (train_data['User-ID'], train_data['ISBN'])), shape=(n_users, n_items))\n",
    "test_matrix = csr_matrix((test_data['Book-Rating'], (test_data['User-ID'], test_data['ISBN'])), shape=(n_users, n_items))\n",
    "print(\"-------matrix finished---------\")\n",
    "\n",
    "if np.any(np.isnan(train_matrix.data)):\n",
    "    print(\"NaN values found in training matrix\")\n",
    "else:\n",
    "    print(\"No NaN values in training matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "users[['Age']] = scaler.fit_transform(users[['Age']])\n",
    "books[['Year-Of-Publication']] = scaler.fit_transform(books[['Year-Of-Publication']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Model\n",
    "We use an embedding-based neural network model to learn representations of users and items. \n",
    "\n",
    "#### Architecture:\n",
    "\n",
    "Embedding Layers:Dense representations for users, books, locations, authors, and publishers.\n",
    "\n",
    "Linear Layers: Transform age, year of publication into embeddings.\n",
    "\n",
    "Concatenation Layer: Combine all embeddings into a single dense vector.\n",
    "\n",
    "Output Layer: Output embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim, num_locations, num_authors, num_publishers):\n",
    "    # def __init__(self, num_users, num_books, embedding_dim, num_authors):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n",
    "        self.location_embedding = nn.Embedding(num_locations, embedding_dim)\n",
    "        self.author_embedding = nn.Embedding(num_authors, embedding_dim)\n",
    "        self.publisher_embedding = nn.Embedding(num_publishers, embedding_dim)\n",
    "        self.user_age = nn.Linear(1, embedding_dim)\n",
    "        self.book_year = nn.Linear(1, embedding_dim)\n",
    "    \n",
    "    def forward(self, user_id, book_id, location_id, age, author_id, year, publisher_id):\n",
    "    # def forward(self, user_id, book_id, age, author_id, year):\n",
    "        user_embed = self.user_embedding(user_id).squeeze()\n",
    "        book_embed = self.book_embedding(book_id).squeeze()\n",
    "        location_embed = self.location_embedding(location_id).squeeze()\n",
    "        author_embed = self.author_embedding(author_id).squeeze()\n",
    "        publisher_embed = self.publisher_embedding(publisher_id).squeeze()\n",
    "        age_embed = self.user_age(age).squeeze()\n",
    "        year_embed = self.book_year(year).squeeze()\n",
    "        return torch.cat([user_embed, book_embed, location_embed, age_embed, author_embed, year_embed, publisher_embed], dim=-1)\n",
    "        # return torch.cat([user_embed, book_embed, author_embed, age_embed, year_embed], dim=-1)\n",
    "\n",
    "# train_data = train_data.merge(users[['User-ID', 'Age']], on='User-ID')\n",
    "# train_data = train_data.merge(books[['ISBN', 'Book-Author', 'Year-Of-Publication']], on='ISBN')\n",
    "train_data = train_data.merge(users[['User-ID', 'Age', 'Location']], on='User-ID')\n",
    "train_data = train_data.merge(books[['ISBN', 'Book-Author', 'Year-Of-Publication', 'Publisher']], on='ISBN')\n",
    "\n",
    "train_data['Age'] = train_data['Age'].apply(lambda x: torch.tensor(x).float().unsqueeze(0))\n",
    "train_data['Book-Author'] = train_data['Book-Author'].apply(lambda x: torch.tensor(x).long())\n",
    "train_data['Year-Of-Publication'] = train_data['Year-Of-Publication'].apply(lambda x: torch.tensor(x).float().unsqueeze(0))\n",
    "train_data['Location'] = train_data['Location'].apply(lambda x: torch.tensor(x).long())\n",
    "train_data['Publisher'] = train_data['Publisher'].apply(lambda x: torch.tensor(x).long())\n",
    "\n",
    "user_ids = torch.tensor(train_data['User-ID'].values).long()\n",
    "book_ids = torch.tensor(train_data['ISBN'].values).long()\n",
    "ratings = torch.tensor(train_data['Book-Rating'].values).float()\n",
    "ages = torch.stack(list(train_data['Age'].values))\n",
    "locations = torch.tensor(train_data['Location'].values.tolist()).long()\n",
    "authors = torch.tensor(train_data['Book-Author'].values.tolist()).long()\n",
    "years = torch.stack(list(train_data['Year-Of-Publication'].values))\n",
    "publishers = torch.tensor(train_data['Publisher'].values.tolist()).long()\n",
    "\n",
    "# dataset = TensorDataset(user_ids, book_ids, ratings, ages, authors, years)\n",
    "dataset = TensorDataset(user_ids, book_ids, ratings, ages, locations, authors, years, publishers)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "The model is trained to minimize the mean squared error (MSE) between the predicted and actual user ratings. \n",
    "We created DataLoader to generate batching and shuffling during training, then updating the model parameters using backpropagation and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "num_users = len(user_id_mapping)\n",
    "num_books = len(book_id_mapping)\n",
    "num_locations = users['Location'].nunique()\n",
    "num_authors = books['Book-Author'].nunique()\n",
    "num_publishers = books['Publisher'].nunique()\n",
    "\n",
    "print(f\"{num_users} users, {num_books} books\")\n",
    "print(f\"dataloader: {len(dataloader)}\")\n",
    "\n",
    "model = EmbeddingNet(num_users, num_books, embedding_dim, num_locations, num_authors, num_publishers)\n",
    "# model = EmbeddingNet(num_users, num_books, embedding_dim, num_authors)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'epoch{epoch} starts:')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        user_id, book_id, rating, age, location_id, author_id, year, publisher_id = batch\n",
    "        \n",
    "        user_id = user_id.long()\n",
    "        book_id = book_id.long()\n",
    "        rating = rating.float()\n",
    "        \n",
    "        embedding = model(user_id, book_id, location_id, age, author_id, year, publisher_id)\n",
    "        # embedding = model(user_id, book_id, age, author_id, year)\n",
    "\n",
    "        output = torch.sum(embedding, dim=1)\n",
    "        # print(f'Rating: {rating}, Output: {output}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # output = embedding.dot(embedding)\n",
    "        loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        # print(f\"{i}/{len(dataloader)} in epoch {epoch}\")\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_data)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'embedding_model.pth')\n",
    "\n",
    "user_embeddings = model.user_embedding.weight.data.numpy()\n",
    "book_embeddings = model.book_embedding.weight.data.numpy()\n",
    "\n",
    "np.save('user_embeddings.npy', user_embeddings)\n",
    "np.save('book_embeddings.npy', book_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation Generation\n",
    "Once the model is trained, we can generate recommendations for users. The process involves:\n",
    "\n",
    "1. Precomputing Book Embeddings:\n",
    "\n",
    "Compute and store embeddings for all books in the dataset to speed up similarity calculations during recommendation.\n",
    "\n",
    "2. Generating Recommendations:\n",
    "\n",
    "For each user, compute the cosine similarity between the user’s embedding and all book embeddings.\n",
    "Sort books by similarity score and select the top N recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_data = test_data.merge(users[['User-ID', 'Age', 'Location']], on='User-ID')\n",
    "test_data = test_data.merge(books[['ISBN', 'Book-Author', 'Year-Of-Publication', 'Publisher']], on='ISBN')\n",
    "\n",
    "test_data['Age'] = test_data['Age'].apply(lambda x: torch.tensor(x).float().unsqueeze(0))\n",
    "test_data['Book-Author'] = test_data['Book-Author'].apply(lambda x: torch.tensor(x).long())\n",
    "test_data['Year-Of-Publication'] = test_data['Year-Of-Publication'].apply(lambda x: torch.tensor(x).float().unsqueeze(0))\n",
    "test_data['Location'] = test_data['Location'].apply(lambda x: torch.tensor(x).long())\n",
    "test_data['Publisher'] = test_data['Publisher'].apply(lambda x: torch.tensor(x).long())\n",
    "\n",
    "user_ids_test = torch.tensor(test_data['User-ID'].values).long()\n",
    "book_ids_test = torch.tensor(test_data['ISBN'].values).long()\n",
    "ratings_test = torch.tensor(test_data['Book-Rating'].values).float()\n",
    "ages_test = torch.stack(list(test_data['Age'].values))\n",
    "locations_test = torch.tensor(test_data['Location'].values.tolist()).long()\n",
    "authors_test = torch.tensor(test_data['Book-Author'].values.tolist()).long()\n",
    "years_test = torch.stack(list(test_data['Year-Of-Publication'].values))\n",
    "publishers_test = torch.tensor(test_data['Publisher'].values.tolist()).long()\n",
    "\n",
    "test_dataset = TensorDataset(user_ids_test, book_ids_test, ratings_test, ages_test, locations_test, authors_test, years_test, publishers_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "The recommendation system is evaluated using precision and recall metrics. The evaluation process includes:\n",
    "\n",
    "1. Defining User Preferences:\n",
    "Use the embeddings of books that the user has rated to define their preferences.\n",
    "\n",
    "2. Calculating Similarity:\n",
    "Compute the cosine similarity between the embeddings of the recommended books and the books the user has rated.\n",
    "\n",
    "3. Evaluating Recommendations:\n",
    "Determine if the recommended books are liked by the user based on the similarity scores.\n",
    "Identify books that the user might like but were not recommended.\n",
    "\n",
    "4. Calculating Metrics:\n",
    "Compute average precision and recall across all users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            user_id, book_id, rating, age, location_id, author_id, year, publisher_id = batch\n",
    "\n",
    "            user_id = user_id.long()\n",
    "            book_id = book_id.long()\n",
    "            rating = rating.float()\n",
    "\n",
    "            embedding = model(user_id, book_id, location_id, age, author_id, year, publisher_id)\n",
    "            output = torch.sum(embedding, dim=1)\n",
    "\n",
    "            loss = criterion(output, rating)\n",
    "            total_loss += loss.item()\n",
    "    return np.sqrt(total_loss / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "num_users = len(user_id_mapping)\n",
    "num_books = len(book_id_mapping)\n",
    "num_locations = users['Location'].nunique()\n",
    "num_authors = books['Book-Author'].nunique()\n",
    "num_publishers = books['Publisher'].nunique()\n",
    "\n",
    "model = EmbeddingNet(num_users, num_books, embedding_dim, num_locations, num_authors, num_publishers)\n",
    "model.load_state_dict(torch.load('embedding_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "book_embeddings = {}\n",
    "for book_id in book_id_mapping.values():\n",
    "    if book_id in books.index:\n",
    "        book_idx = torch.tensor([book_id]).long()\n",
    "        author_id = torch.tensor([books.loc[book_id, 'Book-Author']]).long()\n",
    "        year = torch.tensor([books.loc[book_id, 'Year-Of-Publication']]).float().unsqueeze(0)\n",
    "        publisher_id = torch.tensor([books.loc[book_id, 'Publisher']]).long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = model.book_embedding(book_idx).squeeze().numpy()\n",
    "            book_embeddings[book_id] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = evaluate_model(model, test_dataloader, criterion)\n",
    "print(f'Test RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision_at_k(actual, predicted, k):\n",
    "#     if len(predicted) > k:\n",
    "#         predicted = predicted[:k]\n",
    "#     return len(set(predicted) & set(actual)) / float(k)\n",
    "\n",
    "# def recall_at_k(actual, predicted, k):\n",
    "#     if len(predicted) > k:\n",
    "#         predicted = predicted[:k]\n",
    "#     return len(set(predicted) & set(actual)) / float(len(actual))\n",
    "\n",
    "def recommend_books(user_index, user_location, user_age, model, num_recommendations=15):\n",
    "    scores = {}\n",
    "    user_embed = model.user_embedding(user_index).squeeze().detach().numpy()\n",
    "    \n",
    "    for book_id, book_embed in book_embeddings.items():\n",
    "        score = cosine_similarity([user_embed], [book_embed])[0][0]\n",
    "        scores[book_id] = score\n",
    "\n",
    "    recommended_books = sorted(scores, key=scores.get, reverse=True)[:num_recommendations]\n",
    "    return recommended_books\n",
    "\n",
    "\n",
    "def evaluate_model2(test_dataloader, model, k=15):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        user_ids, book_ids, ratings, ages, locations, authors, years, publishers = batch\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            user_index = user_ids[i].unsqueeze(0)\n",
    "            user_location = locations[i].unsqueeze(0)\n",
    "            user_age = ages[i].unsqueeze(0)\n",
    "            \n",
    "            actual_books = test_data[test_data['User-ID'] == user_id.item()]['ISBN'].values\n",
    "            if len(actual_books) == 0:\n",
    "                continue\n",
    "            recommended_books = recommend_books(user_index, user_location, user_age, model, num_recommendations=k)\n",
    "            actual_book_embeddings = np.array([book_embeddings[book_id_mapping[book]] for book in actual_books if book in book_id_mapping])\n",
    "            recommended_book_embeddings = np.array([book_embeddings[book_id] for book_id in recommended_books])\n",
    "            if actual_book_embeddings.size == 0 or recommended_book_embeddings.size == 0:\n",
    "                continue\n",
    "            \n",
    "            similarities = cosine_similarity(recommended_book_embeddings, actual_book_embeddings)\n",
    "            \n",
    "            liked_books = (similarities > 0.6).sum(axis=1) > 0  # Define liked books based on similarity threshold\n",
    "            \n",
    "            precision = liked_books.sum() / len(recommended_books)\n",
    "            recall = liked_books.sum() / len(actual_books)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    \n",
    "    return avg_precision, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision, avg_recall = evaluate_model2(test_dataloader, model, k=15)\n",
    "print(f'Average Precision@15: {avg_precision}')\n",
    "print(f'Average Recall@15: {avg_recall}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
